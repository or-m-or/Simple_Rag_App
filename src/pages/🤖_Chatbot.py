import streamlit as st
import openai
from llama_index.llms.openai import OpenAI
try:
    from llama_index import VectorStoreIndex, ServiceContext, Document, SimpleDirectoryReader
except ImportError:
    from llama_index.core import VectorStoreIndex, ServiceContext, Document, SimpleDirectoryReader
from config import config
# import nest_asyncio
# nest_asyncio.apply()


@st.cache_resource(show_spinner=False)
def initialize_index(model, temperature, documents_dir=config["input_directory"]):
    reader = SimpleDirectoryReader(input_dir=documents_dir, recursive=True)
    docs = reader.load_data()
    
    PROMPT = '''
            ë‹¹ì‹ ì€ KT AIVLE Schoolì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ í•˜ëŠ” ìƒë‹´ì‚¬ì´ë©° AIVLE Schoolê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µí•˜ëŠ” ê²ƒì´ ë‹¹ì‹ ì˜ ì„ë¬´ì…ë‹ˆë‹¤.
            ë‹µë³€ì€ ì‚¬ì‹¤ì— ê·¼ê±°í•´ì•¼ í•˜ë©°, ì–´ë– í•œ í™˜ìƒë„ í¬í•¨ë˜ì–´ì„œëŠ” ì•ˆë©ë‹ˆë‹¤.
            
            ë¬¸ì„œ ì•ˆì— í¬í•¨ëœ ë‚´ìš©ì„ í™œìš©í•´ì„œ ìµœëŒ€í•œ ìì„¸íˆ ëŒ€ë‹µí•´ ì£¼ì„¸ìš”.
            ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•©ë‹ˆë‹¤.
        '''
        
    llm = OpenAI(model=model, temperature=temperature, system_prompt=PROMPT)
    service_context = ServiceContext.from_defaults(llm=llm)
    index = VectorStoreIndex.from_documents(
        docs, service_context=service_context
    )
    return index


def main_page():
    st.set_page_config(
        page_icon="ğŸ¤–",
        page_title="Build a RAGs bot - Chatbot",
        layout="centered",
        initial_sidebar_state="expanded",
        menu_items=None,
    )
    st.title("Simple RAG Chat Example")
    st.info(
        "ì±„íŒ…ì„ ì‹œì‘í•˜ê¸° ì „ì— ì•„ë˜ í•­ëª©ì„ ë¨¼ì € ìˆ˜í–‰í•´ì£¼ì„¸ìš”! \n"
        "1. Setup í˜ì´ì§€ì˜ [LLM ì„¤ì •]ì—ì„œ OpenAIì˜ api keyë¥¼ ë¨¼ì € ê¸°ì…í•´ì£¼ì„¸ìš”. \n"
        "2. Setup í˜ì´ì§€ì˜ [ë¬¸ì„œ ì—…ë¡œë“œ]ì—ì„œ í…ŒìŠ¤íŠ¸ í•´ë³´ê³  ì‹¶ì€ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”. \n\n"
        "ê·¸ëŸ° ë‹¤ìŒ, ì—…ë¡œë“œí•œ ë¬¸ì„œì— ëŒ€í•œ ì±„íŒ…ì„ ì‹œì‘í•´ë³´ì„¸ìš”",
        icon="ğŸ’¬"
    )
    
    if not st.session_state.get('api_key', ''):
        st.error("ğŸš¨ Setup page ì—ì„œ ë¨¼ì € API key ë¥¼ ê¸°ì…í•˜ê³  í…ŒìŠ¤íŠ¸ í•˜ê³ ì í•˜ëŠ” ë¬¸ì„œë¥¼ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”")
        return  # Stop further execution of the function if required settings are missing


    # ì±„íŒ… ë©”ì„¸ì§€ ê¸°ë¡ ì´ˆê¸°í™”
    if "messages" not in st.session_state.keys():
        st.session_state.messages = [
            {"role": "assistant", "content": "ì—…ë¡œë“œí•œ ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸ì„ ì‹œì‘í•´ì£¼ì„¸ìš”!"}
        ]
    
    # ì¸ë±ìŠ¤ ë° ì±„íŒ… ì—”ì§„ ì´ˆê¸°í™”
    if "index" not in st.session_state:
        st.session_state.index = initialize_index(st.session_state.model_name, st.session_state.model_temperature)

    if "chat_engine" not in st.session_state:
        st.session_state.chat_engine = st.session_state.index.as_chat_engine(chat_mode="condense_question", verbose=True)

    # ì‚¬ìš©ì ì…ë ¥ í”„ë¡¬í”„íŠ¸ ë° ì±„íŒ… ê¸°ë¡ ì„¸ì…˜ì— ì €ì¥
    if prompt := st.chat_input("Your question"):
        st.session_state.messages.append({"role": "user", "content": prompt})

    # ì´ì „ ì±„íŒ… ê¸°ë¡ ì¶œë ¥
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])
    
    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ ì–´ì‹œìŠ¤í„´íŠ¸ê°€ ë³´ë‚¸ ë©”ì‹œì§€ê°€ ì•„ë‹Œ ê²½ìš° ìƒˆ ì‘ë‹µì„ ìƒì„±
    if st.session_state.messages and st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = st.session_state.chat_engine.chat(prompt)
                st.write(response.response)
                message = {"role": "assistant", "content": response.response}
                st.session_state.messages.append(message) # ë©”ì„¸ì§€ ê¸°ë¡ì— ìƒˆë¡œìš´ ì‘ë‹µ ì¶”ê°€
    


if __name__=="__main__":
    main_page()